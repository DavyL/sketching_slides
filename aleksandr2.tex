
\begin{frame}{Parallel with signal processing}
		
		\begin{itemize}
			\item Recall that in signal processing our goal is to reconstruct a vector $x \in \mathbb{R}^{d}$ from $y = Ax + \epsilon$
			\item At first glance compressive learning setup is rather different as we deal with a large collection of vectors, rather than just one
			\item The analogy becomes clearer if we assume (as it is often the case in ML) that our vectors {$\{x_{i}\}_{i = 1}^{n}$} are modeled as i.i.d. random vectors having a probability measure $\mathbb{P}$
			
			\item In this case we get
			$$
			\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^{n} \Phi\left(x_{i}\right) \stackrel{a.s.}{=} \mathbb{E_{\mathbb{P}}}[\Phi(X)] = \mathcal{A}(\mathbb{P}),
			$$
			where $\mathcal{A}$ is a linear operator matching a probability measure to the expectation over this measure of the feature map $\Phi$
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Parallel with signal processing}
		\begin{itemize}
			\item In this manner we can write
			$$
			\tilde{z} =  \frac{1}{n} \sum_{i=1}^{n} \Phi(x_{i}) \approx \mathcal{A}(\mathbb{P}) = \mathcal{A}(\mathbb{P}) + \epsilon
			$$
			\item Thus instead of considering a linear projection of a vector measured with noise ($x \rightarrow Ax + \epsilon$), in compressive learning we consider a linear projection of the underlying probability measured with noise: $\mathbb{P} \rightarrow \mathcal{A}(\mathbb{P}) + \epsilon$
			
			\item In signal processing the linear measurement matrix A can be chosen at random to ensure good reconstruction properties with high probability. By analogy, in compressive sensing $\Phi$ is also often randomised
		\end{itemize}
	\end{frame}

