\section{Examples}
\begin{frame}{Principal Component Analysis (PCA)}
	\begin{block}{PCA}
		The goal is to find the linear subspace $P_k$ that best fits the $d$-dimensional data $\{x_i\}_{i=1}^N$ in the LS sense, i.e., find an orthogonal family of $k$ vectors $\{u_l\}_{l=1}^k$ that maximizes
		\begin{equation*}
				\sum_{l=1}^k\sum_{i=1}^N |u_l^Tx_i|^2.
		\end{equation*}
	\end{block}
	A solution is the $k$-principal eigenvectors of the empirical autocorrelation matrix
	\begin{equation*}
		\hat{R} = \frac{1}{N}\sum_{i=1}^N x_ix_i^T =: \frac{1}{N}\sum_{i=1}^N \Phi(x_i).
	\end{equation*}
	$\hat{R}$ is a \emph{sketch} of our data (of dim $d^2$).
\end{frame}


\begin{frame}
	The sketch 
	\begin{equation*}
		\hat{R} = \frac{1}{N}\sum_{i=1}^N x_ix_i^T =: \frac{1}{N}\sum_{i=1}^N \Phi(x_i) \in \mathbb{R}^{d^2}
	\end{equation*}
 	is a very compressed version of the data $\{x_i\}_{i=1}^N$, \emph{but}, it still contains the geometry of the data.
	\begin{block}{CS inspired idea}
		Take $m$ random measurements\footnote{$\mathcal{M}:\mathbb{R}^{d\times d}\to \mathbb{R}^m$ satisfying RIP on matrices of rank at most $2k$.} of each sample and use the sketch defined by $\Phi(x) = \mathcal{M}(xx^T)$. Provided $m>kd$, the principal eigenvectors can be recovered.
	\end{block}

\end{frame}

\begin{frame}{$k$-means centroids}
	\begin{block}{The problem}
		The goal is to recover $k$ centroids $\{c_l\}_{l=1}^k$ from some data $\{x_i\}_{i=1}^N$ that minimize
		\begin{equation*}
			\sum_{i=1}^N \min_l ||x_i - c_l||^2.
		\end{equation*}
	\end{block}
	For $N>>1$ traditional algorithms are not very efficient because they take the whole dataset at once...
	\newline
	\emph{But}, $N>>1$ allows to use the laws of large numbers and concentration. It is reasonable to consider that the data will accumulate on small portions of the space.

\end{frame}

\begin{frame}
	\begin{block}{The binning map}
		Assume the centroids are spaced by at least $\varepsilon$ and have a norm smaller than $r$, then cover $[-r,r]^d$ by, $N=(\frac{2r}{\varepsilon})^d$, $d$-dimensional cubes (\emph{bins}). For each bin, count the average number of points that belong to it. This defines the binning map $\hat{p}\in\mathbb{R}^N_+$.
	\end{block}
	This gives us a sketch of the data, but in a large dimensional space.\newline
	\emph{But}, if the model that generated the data is "structured", i.e., the data concentrates in a few centroids, then the problem is a \emph{sparse} problem.
	\begin{block}{CS inspired idea}
		Use a Gaussian random matrix in $\mathbb{R}^{m\times N}$ and define the sketch as $\tilde z = A\hat{p}$ and solve
		\begin{equation*}
			\tilde p = argmin_{p\in\Sigma_k^+}||\tilde z - Ap||^2\quad (+\lambda||p||_1)
		\end{equation*}

	\end{block}


\end{frame}

