\section{Examples}
\begin{frame}{Principal Component Analysis (PCA)}
	\begin{block}{PCA}
		The goal is to find the linear subspace $P_k$ that best fits the $d$-dimensional data $\{x_i\}_{i=1}^N$ in the LS sense, i.e., find an orthogonal family of $k$ vectors $\{u_l\}_{l=1}^k$ that maximizes
		\begin{equation*}
				\sum_{l=1}^k\sum_{i=1}^N |u_l^Tx_i|^2.
		\end{equation*}
	\end{block}
	A solution is the $k$-principal eigenvectors of the empirical autocorrelation matrix
	\begin{equation*}
		\hat{R} = \frac{1}{N}\sum_{i=1}^N x_ix_i^T =: \frac{1}{N}\sum_{i=1}^N \Phi(x_i).
	\end{equation*}
	$\hat{R}$ is a \emph{sketch} of our data (of dim $d^2$).
\end{frame}


\begin{frame}
	The sketch 
	\begin{equation*}
		\hat{R} = \frac{1}{N}\sum_{i=1}^N x_ix_i^T =: \frac{1}{N}\sum_{i=1}^N \Phi(x_i) \in \mathbb{R}^{d^2}
	\end{equation*}
 	is a very compressed version of the data $\{x_i\}_{i=1}^N$, \emph{but}, it still contains the geometry of the data.
	\begin{block}{CS inspired idea}
		Take $m$ random measurements\footnote{$\mathcal{M}:\mathbb{R}^{d\times d}\to \mathbb{R}^m$ satisfying RIP on matrices of rank at most $2k$.} of each sample and use the sketch defined by $\Phi(x) = \mathcal{M}(xx^T)$. Provided $m>kd$, the principal eigenvectors can be recovered.
	\end{block}

\end{frame}

\begin{frame}{$k$-means centroids}
	\begin{block}{The problem}
		The goal is to recover $k$ centroids $\{c_l\}_{l=1}^k$ from some data $\{x_i\}_{i=1}^N$ that minimize
		\begin{equation*}
			\sum_{i=1}^N \min_l ||x_i - c_l||^2.
		\end{equation*}
	\end{block}
	For $N>>1$ traditional algorithms are not very efficient because they take the whole dataset at once...
	\newline
	\emph{But}, $N>>1$ allows to use the laws of large numbers and concentration. It is reasonable to consider that the data will accumulate on small portions of the space.

\end{frame}

\begin{frame}
	\begin{block}{The binning map}
		Assume the centroids are spaced by at least $\varepsilon$ and have a norm smaller than $r$, then cover $[-r,r]^d$ by, $N=(\frac{2r}{\varepsilon})^d$, $d$-dimensional cubes (\emph{bins}). For each bin, count the average number of points that belong to it. This defines the binning map $\hat{p}\in\mathbb{R}^N_+$.
	\end{block}
	This gives us a sketch of the data, but in a large dimensional space.\newline
	\emph{But}, if the model that generated the data is "structured", i.e., the data concentrates in a few centroids, then the problem is a \emph{sparse} problem.
	\begin{block}{CS inspired idea}
		Use a Gaussian random matrix in $\mathbb{R}^{m\times N}$ and define the sketch as $\tilde z = A\hat{p}$ and solve
		\begin{equation*}
			\tilde p = argmin_{p\in\Sigma_k^+}||\tilde z - Ap||^2 
		\end{equation*}
	\end{block}
\end{frame}

\begin{frame}{Sketching estimates the underlying data-distribution}
	We can do $k$-means clustering with $m$ measures using:
		\begin{itemize}
			\item $m\geq k\log N$ for Gaussian sampling
			\item $m \geq k\log(k)^3\log N$ for DFT of $\hat{p}$
		\end{itemize}
	What if we consider the \emph{continuous} Fourier Tranform (FT) ?
	\newline
	We only know the empirical distribution $\bar p_\mathcal{X} = \frac{1}{N}\sum_{i=1}^N \delta(x_i - x)$, so
	\begin{align*}
		FT(\bar p)(\omega) &= \int_{\mathbb{R}^d} \bar p_\mathcal{X}(x) e^{-i2\pi\langle w, x\rangle}dx = \frac{1}{N}\sum_{i=1}^N e^{-i2\pi\langle w,x\rangle} =: \bar \Psi_{\bar p_\mathcal{X}}(\omega) \\
				&\longrightarrow^{\mathbb{E}} \bar \Psi_{p^*}(\omega) \quad\text{The "true" characteristic function at $\omega$}.
	\end{align*}
	\begin{block}{CS inspired idea}
		If the true distribution $p^*$ is "simple", then interpolating the characteristic function, using simple models, on "few" of its samples should give the true distribution.
	\end{block}
\end{frame}

