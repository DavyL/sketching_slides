
% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This file is a template using the "beamer" package to create slides for a talk or presentation
% - Giving a talk on some subject.
% - The talk is between 15min and 45min long.
% - Style is ornate.

% MODIFIED by Jonathan Kew, 2008-07-06
% The header comments and encoding in this file were modified for inclusion with TeXworks.
% The content is otherwise unchanged from the original distributed with the beamer package.

\documentclass{beamer}
\newtheorem{proposition}{Proposition}


% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 


\mode<presentation>
{
  \usetheme{Warsaw}
  % or ...

  \setbeamercovered{transparent}
  % or whatever 
}


\usepackage[english]{babel}
% or whatever

\usepackage[utf8]{inputenc}
% or whatever
\usepackage{mathrsfs}  
\usepackage{graphicx}
\usepackage{times}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{subcaption}
\usepackage{tikz}

% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.


\title[Sketching and compressive learning]{Sketching : A framework for compressive learning}

\author[Davy, Gjorgjevski, Pak] % (optional, use only with lots of authors)
{Leo Davy \and Martin Gjorgjevski \and Aleksandr Pak}
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[ENS Lyon] % (optional, but mostly needed)
{
  ENS Lyon \\
  M2 Advanced Mathematics}
% - Use the \inst command only if there are several affiliations.
% - Keep it tildeple, no one is interested in your street address.

\date[Short Occasion] % (optional)
{March 2022}

\defbeamertemplate*{footline}{shadow theme}
{%
  \leavevmode%
  \hbox{\begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm plus1fil,rightskip=.3cm]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertframenumber\,/\,\inserttotalframenumber\hfill\insertshortauthor%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle%
  \end{beamercolorbox}}%
  \vskip0pt%
}
\setbeamertemplate{headline}
{%
  \leavevmode%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex]{section in head/foot}%
    \hbox to .5\paperwidth{\hfil\insertsectionhead\hfil}%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex]{subsection in head/foot}%
    \hbox to .5\paperwidth{\hfil\insertsubsectionhead\hfil}%
  \end{beamercolorbox}%
}
\setbeamertemplate{navigation symbols}{} %
\subject{Talks}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{aleksandr1}
\input{leo1}

\input{leo2}
\input{aleksandr2}

\begin{frame}{Task driven distances}
	\begin{block}{Task driven distance}
		Given a loss function $L$ and probability distributions $p_X$ and $p_X^'$, we consider the distance 
		
		\begin{equation*}
		    \rho(p_X,p_{X}^{'} )=\sup_{\theta}|R^*(\theta|p_X)-R^*(\theta|p_{X}^{'})|  
		\end{equation*}
		where $R^*(\theta|p_X)=E_{X\sim p_X}(L(\theta)|X)$ is the expected risk under $p_X$.
	
	\end{block}
	\begin{itemize}
	    \item The loss function $L$ is task specific
	    \item Excess risk bounds $0\leq R^*(\theta^{*'}|p_X)-R^*(\theta^{*}|p_X)\leq 2\rho(p_X,p_{X}^{'}) $ where $\theta^*=argmin_{\theta} R^*(\theta|p_X)$ and $\theta^{*'}=argmin_{\theta} R^*(\theta|p_{X}^{'})$
	    \item When $\hat{p}_X$ is the empirical distribution on the data, and $p_X$ is the true distribution, under certain conditions $\rho(\hat{p}_X,p_X)=O(\frac{1}{\sqrt{n}})$.
	\end{itemize}


\end{frame}

\begin{frame}
In the compressive learning framework we are interested in an upper bound of the excess risk which is controlled by the task driven distance.
	\begin{block}{The Lower Restricted Isometry Property (LRIP) } 
		The operator $\mathcal{A}$ is said to have the LRIP with constant $C_0$ and with respect to a parametric subfamily $\Sigma_{\theta}=\{p_{\theta}|\theta\in\Theta\}$ if
		\begin{equation*}
		    \rho(p_{\theta},p_{\theta^{'}})\leq C_0 ||\mathcal{A}(p_{\theta})-\mathcal{A}(p_{\theta^{'}})||
		\end{equation*}
		for all $p_{\theta},p_{\theta^{'}}\in\Sigma_{\theta}$
	\end{block}
	
	Excess risk bound under LRIP:  for all $\theta^{'}\in\Sigma_{\theta}$ 
    \begin{equation*}
    \begin{split}
        &R^*(\tilde{\theta}|p_X)-R^*(\theta^{'}|p_X)\leq 4C_0||\mathcal{A}(p_X)-\tilde{z}||\\
        &+4C_0||\mathcal{A}(p_{\theta^{'}})-\mathcal{A}(p_X)||+2\rho(p_{\theta^{'}},p_X)
    \end{split}
    \end{equation*}
with $\tilde{\theta}=argmin_{\theta}C(\theta|\tilde{z}) $

\begin{itemize}
    \item Choosing $\theta^{'}=\theta^*$, this result is interpretable in terms of modeling and sampling error
\end{itemize}
\end{frame}

\begin{frame}{Expected and mean kernel, MMD}
\begin{itemize}
    \item Two sources of randomness: the data and the random features used for the sketch
    \item For the random feature map we have $<\frac{1}{m}\Phi(x),\frac{1}{m}\Phi(x^{'})>=\frac{1}{m}\sum_{j=1}^m e^{-j2\pi w_j(x-x^{'})}$
    \item Averaging over the random features gives the \emph{expected kernel} $k(x,x^{'})=E_w(\exp(-j2\pi<w,x-x^{'}>))$
    \item We define the \emph{mean kernel} as
    $k(p,q)=E_{x\sim p, x^{'}\sim q} k(x,x^{'})$
    \item A quantity of interest is the \emph{maximum mean discrepancy}  $MMD(p,q)=\sqrt{k(p,p)-2k(p,q)+k(q,q)}$
    \item It can be shown using concentration of measure that
     $\frac{1}{\sqrt{2}}MMD(p_{\theta},p_{\theta^{'}})\leq \frac{1}{\sqrt{m}}||\mathcal{A}(p_{\theta})-\mathcal{A}(p_{\theta^{'}})||$ when $\Sigma_{\theta}$ is a finite set
     \item When $\Sigma_{\theta}$ is infinite additional assumptions are required to ensure that the LRIP property holds
\end{itemize}

\end{frame}

\begin{frame}{Compressed clustering, fast sketching}
\begin{itemize}
    \item In practice computing the sketch using random Fourier samples might be problematic (accurate implementations of the complex valued function $x\rightarrow\exp(-j2\pi x)$ is slow)
    \item Using the Fast Walsh-Hadamard transform it is possible to speed up the sketching process ($\Theta(d\log d)$ instead of $\Theta(d^2)$ for matrix products)  
    \item Empirical studies on simulated and real data show that with proper setup, learning from sketch is comparable to classical approaches (k-means clustering) 
\end{itemize}
	\center
	\includegraphics[ width=\textwidth]{fig12}
\end{frame}

\begin{frame}{Conclusion}
	Sketching is a framework that allows:
	\begin{itemize}
		\item to get leverage of assumptions on the simplicity of the underlying data generating process
		\item to apply ideas similar to Compressed Sensing
		\item learn the underlying data distribution through a compressed dataset
		\item not to reduce the dimension of the features, but the dimension of the dataset, without losing its characteristic (geometric) properties
	\end{itemize}
	\center
	\includegraphics[height=0.4\textheight, width=0.7\textwidth]{CS}
\end{frame}



\end{document}